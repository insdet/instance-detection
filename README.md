<div align="center">
<h1>A High-Resolution Dataset for Instance Detection with Multi-View Instance Capture</h1>

[**Qianqian Shen**](https://shenqq377.github.io/)<sup>1</sup> · [**Yunhan Zhao**](https://yunhan-zhao.github.io/)<sup>2</sup> ·  [**Nahyun Kwon**](https://nahyunkwon.github.io/)<sup>3</sup> · [**Jeeeun Kim**](https://github.com/qubick)<sup>3</sup> · [**Yanan Li**](https://yananlix1.github.io/)<sup>1</sup> · [**Shu Kong**](https://aimerykong.github.io/)<sup>3</sup><sup>,</sup><sup>4</sup><sup>,</sup><sup>5</sup>

<sup>1</sup>Zhejiang Lab&emsp;<sup>2</sup>UC Irvine&emsp;<sup>3</sup>Texas A&M University&emsp;<sup>4</sup>University of Macau&emsp;<sup>5</sup>Institute of Collaborative

<a href="[https://arxiv.org/abs/2310.19257](https://arxiv.org/abs/2310.19257)"><img src='https://img.shields.io/badge/arXiv-InsDet-red' alt='Paper PDF'></a>
<a href="[https://github.com/insdet/instance-detection/](https://github.com/insdet/instance-detection/)"><img src='https://img.shields.io/badge/Project_Page-InsDet-green' alt='Project Page'></a>
<a href="[InsDet-FULL](https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP?usp=sharing)"><img src='https://img.shields.io/badge/Benchmark-InsDet-yellow' alt='Benchmark'></a>
<!-- <a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-V2'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a> -->
</div>

The paper has been accepted by **NeurIPS (Datasets and Benchmarks) 2023**.

![InsDet](assets/InsDet.png)

## Dataset
The InsDet datase is a high-resolution real-world dataset for **Instance Detection** with **Multi-view Instance Capture**.<br>
We provide an [InsDet-mini](https://drive.google.com/drive/folders/1X8MT5JuLq0Vjq1jNE1I9h3q_JGolNJsI?usp=sharing) for demo and visualization, and the full dataset [InsDet-FULL](https://drive.google.com/drive/folders/1rIRTtqKJGCTifcqJFSVvFshRb-sB0OzP?usp=sharing).

The full dataset contains 100 objects with multi-view profile images in 24 rotation positions (per 15&deg;), 160 testing scene images with high-resolution, and 200 pure background images. The mini version contains 5 objects, 10 testing scene images, and 10 pure background images.

### Details
The **Objects** contains:
- 000_aveda_shampoo
  - images: raw RGB images (e.g., "images/001.jpg")
  - masks: segmentation masks generated by [GrabCut Annotation Toolbox](https://github.com/Kazuhito00/GrabCut-Annotation-Tool) (e.g., "masks/001.png")
- <p align="left"> $\vdots$ </p>
- 099_mug_blue

![vis-objects](/assets/vis-objects.png)

Tip: The first three digits specify the instance id.

The **Scenes** contains:
- easy
  - leisure\_zone
    - raw RGB images with 6144×8192 pixels (e.g. “office001/rgb\_000.jpg”)
    - bounding box annotation for objects in test scenes generated by labelImg toolbox and using PascalVOC format (e.g. “office\_001/rgb\_000.xml”)
  - meeting\_room
  - office\_002
  - pantry\_room\_002
  - sink
- hard
  - office\_001
  - pantry\_room\_001

![vis-scenes](/assets/vis-scenes.png)

Tip: Each bounding box is specified by [xmin, ymin, xmax, ymax].

The **Background** contains 200 pure background images that do not include any instances from **Objects** folder.

![vis-background](/assets/vis-background.png)

## Code
The project is built on [detectron2](https://github.com/facebookresearch/detectron2), [segment-anything](https://github.com/facebookresearch/segment-anything), and [DINOv2](https://github.com/facebookresearch/dinov2).<br>
<!-- Detectron2 provides end-to-end detectors implementation and metric evaluation. Segment-anything is an off-the-shelf class-agnostic segmentation model that we used to produce instance proposals. DINOv2 is a self-supervised vision foundation model that we used to extract feature representation. -->

<!-- ### Data preparation
All profile images in InsDet-Objects are preprocessed by using `minify`, `resizemask`, `getbbox`, `centercrop`, and `invertmask` packed in `gendata/data_utils.py`. Examples for single or loop operation are included in `gendata`. -->

### Demo
The Jupyter notebooks files demonstrate our non-learned method using SAM and DINOv2. We choose light pretrained models of SAM (vit_l) and DINOv2 (dinov2_vits14) for efficiency.
<!-- |  Pretrained Model  | # of params |  AP  | AP50 | AP75 |
| :---               | :---:       | :---:| :---:| :---:|
| ViT-S/14 distilled | 21M         |41.61 |49.10 |45.95 |
|ViT-B/14 distilled  | 86M         |41.89 |49.39 |46.30 |
|ViT-L/14 distilled  | 300M        |43.33 |50.80 |47.84 |
|ViT-g/14            | 1,100M      |44.65 |53.47 |49.11 | -->

## Citation
If you find our project useful, please consider citing:
```bibtex
@inproceedings{shen2023high,
        title={A high-resolution dataset for instance detection with multi-view object capture},
        author={Shen, Qianqian and Zhao, Yunhan and Kwon, Nahyun and Kim, Jeeeun and Li, Yanan and Kong, Shu},
        booktitle={NeurIPS Datasets & Benchmark Track},
        year={2023}
```
